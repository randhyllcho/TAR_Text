---
title: "TAR Course Analysis"
author: "Ryan Christensen"
output:
  pdf_document:
    toc: yes
  html_document:
    theme: cosmo
    toc: yes
    toc_float: yes
---

##Loading the packages and building the url  
```{r collapse=TRUE, message=FALSE}
library(rvest)
library(tidytext)
library(tidyverse)
library(topicmodels)
```

```{r}
urlBase <- sprintf("https://e-discoveryteam.com/tar-course/tar-course-")

target <- map(1:17, function(i) {
  if (i == 1) {
    paste(urlBase, i, "st-class/", sep = "")
  } else if (i == 2) {
    paste(urlBase, i, "nd-class/", sep = "")
  } else if ( i == 3) {
    paste(urlBase, i, "rd-class/", sep = "")
  } else {
    paste(urlBase, i, "th-class/", sep = "")
  }
})
```
## *Borrowing* some data  
Utilizing purrr's `map` function I am able to loop over my `target` list and plug each element into rvest's `read_html()` function. This returns a list of pages that I can then loop over again to get a list of the text elements I'm after.  
```{r}
texts <- map(target, read_html)

text_list <- map(texts, function(i) {
  (html_text(html_nodes(i, ".entry")))
})
```
## Converting *borrowed* data from a list to a Data_Frame  
Tokenizing the text -- using the Tidy Text method `unnest_tokens()` -- arranges the data into a Tidy format of one term per row. This makes removing standard stop words easy with an `anti-join`.    
```{r}
text_df <- data_frame(text = unlist(text_list), Article = 1:17)

text_df <- text_df %>% 
  unnest_tokens(word, text)

text_df %>% 
  anti_join(stop_words) %>% 
  count(word, sort = TRUE)
```
## Taking a look at the authors state of mind   
Simple sentiment analysis of the courses. Each class is chuncked together and plotted with each word's sentiment score colored by degree of positive or negative.
```{r}
text_sentiment <- text_df %>%
  anti_join(stop_words) %>% 
  inner_join(get_sentiments("bing")) %>% 
  count(word, index = row_number() %/% 104, sentiment) %>% 
  spread(sentiment, n, fill = 0) %>% 
  mutate(sentiment = positive - negative)

ggplot(text_sentiment, aes(index, sentiment, fill = sentiment)) +
  geom_col() +
  geom_hline(aes(yintercept = 0), alpha = 0.23, col = "navy") +
  theme(panel.background = element_blank(),
        text = element_text(family = "mono", color = "navy"),
        axis.ticks = element_blank(),
        axis.text.x = element_blank()) +
  ggtitle("Sentiment by Class ")
```

## TF_IDF --- Looking at what terms are most important to the overall text. 
```{r}
article_words <- text_df %>% 
  count(Article, word, sort = TRUE)

total_words <- article_words %>% 
  group_by(Article) %>% 
  summarise(total = sum(n))

article_words <- left_join(article_words, total_words)

article_words
```
```{r message=FALSE, warning=FALSE}
ggplot(article_words, aes(n/total)) +
  geom_histogram(fill = "navy", show.legend = FALSE) +
  facet_wrap(~Article, ncol = 4, scales = "free_y") +
  xlim(NA, 0.04) +
  theme(panel.background = element_blank())

article_words <- article_words %>% 
  bind_tf_idf(word, Article, n)
```
## Stop Words  
Creating a list of custom stop words requires nothing more than a column in a data frame. Here I'm choosing to pull out some random numbers and the `_____` that were used to break sections of each article. 
```{r}
myStopWords <- data_frame(word = c("_____", "193", "502", "d", "95", "2.5", "2006", "california", "ralphâ€™s"))
```

```{r}
article_words %>% 
  anti_join(myStopWords) %>% 
  select(-total) %>% 
  arrange(desc(tf_idf)) %>% 
  top_n(20)

plot_article <- article_words %>% 
  anti_join(myStopWords) %>% 
  arrange(desc(tf_idf)) %>% 
  mutate(word = factor(word, levels = rev(unique(word))))

plot_article %>% 
  top_n(20) %>% 
  ggplot(aes(word, tf_idf, fill = factor(Article))) +
  geom_col()+
  scale_fill_discrete(guide = guide_legend(title = "Article")) +
  coord_flip() +
  theme(panel.background = element_blank(),
        text = element_text(family = "mono", color = "navy"),
        axis.ticks = element_blank())


```